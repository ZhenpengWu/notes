\BOOKMARK [1][-]{section.1}{Erlang}{}% 1
\BOOKMARK [1][-]{section.2}{Reduce}{}% 2
\BOOKMARK [1][-]{section.3}{Scan}{}% 3
\BOOKMARK [1][-]{section.4}{Shared Memory Multiprocessors - Parallel Models}{}% 4
\BOOKMARK [2][-]{subsection.4.1}{The MESI protocol}{section.4}% 5
\BOOKMARK [3][-]{subsubsection.4.1.1}{Implementing MESI: Snooping}{subsection.4.1}% 6
\BOOKMARK [3][-]{subsubsection.4.1.2}{Implementing MESI: Directories}{subsection.4.1}% 7
\BOOKMARK [1][-]{section.5}{Message Passing Computers - Parallel Models}{}% 8
\BOOKMARK [2][-]{subsection.5.1}{Performance Consideration}{section.5}% 9
\BOOKMARK [2][-]{subsection.5.2}{Network Topologies}{section.5}% 10
\BOOKMARK [2][-]{subsection.5.3}{Diameter, Bisection Bandwidth, and Port Number of Each Network}{section.5}% 11
\BOOKMARK [3][-]{subsubsection.5.3.1}{Ring Network}{subsection.5.3}% 12
\BOOKMARK [3][-]{subsubsection.5.3.2}{Star Networks}{subsection.5.3}% 13
\BOOKMARK [3][-]{subsubsection.5.3.3}{Meshes}{subsection.5.3}% 14
\BOOKMARK [3][-]{subsubsection.5.3.4}{Tori}{subsection.5.3}% 15
\BOOKMARK [2][-]{subsection.5.4}{Hypercubes}{section.5}% 16
\BOOKMARK [3][-]{subsubsection.5.4.1}{Trees}{subsection.5.4}% 17
\BOOKMARK [3][-]{subsubsection.5.4.2}{Fat Trees}{subsection.5.4}% 18
\BOOKMARK [1][-]{section.6}{Superscalar Processors: Computer Architecture - Parallel Models}{}% 19
\BOOKMARK [2][-]{subsection.6.1}{Register Renaming}{section.6}% 20
\BOOKMARK [2][-]{subsection.6.2}{Branch Prediction / Speculative Execution}{section.6}% 21
\BOOKMARK [2][-]{subsection.6.3}{Superscalar Execution}{section.6}% 22
\BOOKMARK [2][-]{subsection.6.4}{Comparisons among Shared Memory, Message Passing, and Superscalar}{section.6}% 23
\BOOKMARK [3][-]{subsubsection.6.4.1}{Shared Memory}{subsection.6.4}% 24
\BOOKMARK [3][-]{subsubsection.6.4.2}{Message Passing}{subsection.6.4}% 25
\BOOKMARK [3][-]{subsubsection.6.4.3}{Superscalar}{subsection.6.4}% 26
\BOOKMARK [2][-]{subsection.6.5}{Parallel Performance, Speedup and Efficiency}{section.6}% 27
\BOOKMARK [2][-]{subsection.6.6}{Amdahl's Law}{section.6}% 28
\BOOKMARK [2][-]{subsection.6.7}{Brent's Lemma}{section.6}% 29
\BOOKMARK [2][-]{subsection.6.8}{Work and Span}{section.6}% 30
\BOOKMARK [2][-]{subsection.6.9}{Super-Linear Speedup}{section.6}% 31
\BOOKMARK [2][-]{subsection.6.10}{Embarrassingly Parallel Problems}{section.6}% 32
\BOOKMARK [1][-]{section.7}{Performance Loss}{}% 33
\BOOKMARK [2][-]{subsection.7.1}{Overhead}{section.7}% 34
\BOOKMARK [2][-]{subsection.7.2}{Limited Parallelism}{section.7}% 35
\BOOKMARK [1][-]{section.8}{Models of Parallel Computation}{}% 36
\BOOKMARK [2][-]{subsection.8.1}{The Parallel Random Access Machine\(PRAM\) Model}{section.8}% 37
\BOOKMARK [2][-]{subsection.8.2}{The Work-Span Model}{section.8}% 38
\BOOKMARK [2][-]{subsection.8.3}{The Candidate Type Architecture \(CTA\) Model}{section.8}% 39
\BOOKMARK [2][-]{subsection.8.4}{The LogP Model}{section.8}% 40
\BOOKMARK [1][-]{section.9}{Energy and Parallel Computing}{}% 41
\BOOKMARK [2][-]{subsection.9.1}{Moore's Law}{section.9}% 42
\BOOKMARK [2][-]{subsection.9.2}{Energy and Time}{section.9}% 43
\BOOKMARK [2][-]{subsection.9.3}{The End? of Moore's Law}{section.9}% 44
\BOOKMARK [2][-]{subsection.9.4}{Why Parallelism Matter?}{section.9}% 45
\BOOKMARK [1][-]{section.10}{Sorting Networks}{}% 46
\BOOKMARK [2][-]{subsection.10.1}{The 0-1 Principle}{section.10}% 47
\BOOKMARK [3][-]{subsubsection.10.1.1}{Bitonic Merge}{subsection.10.1}% 48
\BOOKMARK [2][-]{subsection.10.2}{Bitonic Sort}{section.10}% 49
\BOOKMARK [1][-]{section.11}{Data Parallel Computing and CUDA}{}% 50
\BOOKMARK [2][-]{subsection.11.1}{GPUs and Data Parallelism}{section.11}% 51
\BOOKMARK [3][-]{subsubsection.11.1.1}{Grids, Blocks, Threads and Warps}{subsection.11.1}% 52
\BOOKMARK [2][-]{subsection.11.2}{CUDA}{section.11}% 53
\BOOKMARK [2][-]{subsection.11.3}{CUDA Memory}{section.11}% 54
\BOOKMARK [2][-]{subsection.11.4}{CUDA Performance Considerations}{section.11}% 55
